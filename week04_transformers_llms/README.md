# ğŸ¤– Week 4: Transformers and LLMs

## ğŸ§  Topics Covered
- Transformer architecture
- Self-attention mechanisms

## ğŸ› ï¸ Engineering Practice
- Hugging Face Transformers
- Tokenizers, model fine-tuning

## ğŸ“Œ Project
Fine-tune a pre-trained language model on a custom dataset

## ğŸ“‚ Folder Structure
 
    week04_transformers_llms
    â”œâ”€â”€ ... 
    â”œâ”€â”€ notebooks               # Jupyter notebooks for experiments
    â”‚   â”œâ”€â”€                     # 
    â”‚   â”œâ”€â”€                     # 
    â”‚   â”œâ”€â”€                     # 
    â”‚   â”œâ”€â”€                     # 
    â”‚   â””â”€â”€ ...                 # etc.
    â”œâ”€â”€ models                  # Pre-trained models
    â”‚   â”œâ”€â”€                     # 
    â”‚   â”œâ”€â”€                     #    
    â”œâ”€â”€ scripts                 # Python scripts for training and evaluation
    â”‚   â”œâ”€â”€                     # 
    â”‚   â”œâ”€â”€                     #    
    â”œâ”€â”€ dataset                 # Dataset files (ignored in .gitignore)
    â”‚   â”œâ”€â”€                     # 
    â”‚   â”œâ”€â”€                     # 
    â””â”€â”€ README.md